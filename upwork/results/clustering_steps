1. Feature normalization

- Integer features: convert to [0,1] using min_max scaler 
Read more about this function at http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html

- Categorical features: one-2- encoder
Read more at http://statsmodels.sourceforge.net/ipdirective/generated/scikits.statsmodels.tools.tools.categorical.html

- Boolean features: keep original values

2. Missing value: replace with mean for integer features and replace with mode for boolean and categorical features
Read more at http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html

3. Feature selection

3.1 Remove features with low and zero variance: "AdOptedIn","AllowiBeacon","HaveUniqueGlobalID","OS","SignIn","EmailExist","UserType" 

3.2 Perform PCA to select the best features: 

The best features are 

AllowPush
AllowGeo
AllowFeaturePush
AllowBT
EmailAddress
RevokePushBefore
UninstalledTF
UninstalledSameday
NumCampaignMatch
NumCrash
InstallDays
PushCount
CorrectQuestion
RevokePushAfterDays
UninstalledAfter
LastUpdateDays

Read more about PCA at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html

4. Determine number of clusters for k-mean algorithm
 
Use Silhoette coefficients (the bigger the better)  
Read more at http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

Here are a value of k and its silhoette coefficients
(3, 0.15419767227232772)
(4, 0.1749548768057555)
(5, 0.16251883723896135)
(6, 0.16043009203494993)
(7, 0.16346565100465482)
(8, 0.12729074920578809)
(9, 0.13692302565338529)
(10, 0.12924594523964311)
(11, 0.12676767327019539)
(12, 0.1241204428781716)
(13, 0.12448054715003889)
(14, 0.1238881926277654)
(15, 0.1209973068278709)
(16, 0.12095143965823443)
(17, 0.12024508046799569)
(18, 0.11391827979012258)
(19, 0.11109333514948599)
Best k = 4 
